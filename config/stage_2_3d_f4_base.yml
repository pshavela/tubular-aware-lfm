model:
  learning_rate: 0.0001
  learning_rate_min: 0
  lr_warmup_steps: 1000
  sample_image_every_n_epochs: 50
  spatial_dims: 3
  latent_resolution: [64, 64, 32]
  embedding_dim: 8
  num_classes: 9
  z_mean: 0.00767916
  scaling_factor: 1.57657742
  atol: 1e-4
  rtol: 1e-2
  spade_cond:
    num_channels: [32, 64, 128]
    zero_out: True

  model_config:
    num_res_blocks: 2
    num_channels: [64, 128, 256, 512]
    attention_levels: [False, False, True, True]
    num_head_channels: [0, 0, 32, 32]
    use_flash_attention: True

data:
  image_train_dir: "../data/256/cta/training"
  image_val_dir: "../data/256/cta/validation"
  label_train_dir: "../data/256/annotation/training"
  label_val_dir: "../data/256/annotation/validation"
  label_one_hot: True
  number_classes: 9
  augment: False
  encode_latents: "pre"
  dataset_type: "persistent"
  persistent_dir: "persistent_no_attn"
  batch_size: 1
  num_workers: 0
  resolution: [256, 256, 128]
  spacing: [0.7, 0.7, 1.05]

processing:
  autoencode:
    checkpoint: "lightning_logs/version_0/checkpoints/epoch=164-step=247500.ckpt"
    normalize: False

callbacks:
  gpustats:
    on_val: False

  modelcheckpoint:
    every_n_epochs: 100
    save_top_k: -1
    save_last: True
